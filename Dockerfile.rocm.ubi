## Global Args ##################################################################
ARG BASE_UBI_IMAGE_TAG
ARG PYTHON_VERSION
ARG VLLM_TGIS_ADAPTER_VERSION

FROM registry.access.redhat.com/ubi9/ubi-minimal:${BASE_UBI_IMAGE_TAG} AS base

ARG PYTHON_VERSION

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

RUN --mount=type=cache,target=/root/.cache/pip \
 microdnf -y update && \
 microdnf install -y --setopt=install_weak_deps=0 --nodocs \
    python${PYTHON_VERSION}-devel \
    python${PYTHON_VERSION}-pip \
    python${PYTHON_VERSION}-wheel && \
    python${PYTHON_VERSION} -m venv $VIRTUAL_ENV && \
    pip install -U pip wheel setuptools uv && \
 microdnf clean all


FROM base AS rocm_base
ARG ROCM_VERSION
ARG PYTHON_VERSION
ARG BASE_UBI_IMAGE_TAG

RUN printf "[amdgpu]\n\
name=amdgpu\n\
baseurl=https://repo.radeon.com/amdgpu/${ROCM_VERSION}/rhel/${BASE_UBI_IMAGE_TAG/-*/}/main/x86_64/\n\
enabled=1\n\
priority=50\n\
gpgcheck=1\n\
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key\n\
[ROCm-${ROCM_VERSION}]\n\
name=ROCm${ROCM_VERSION}\n\
baseurl=https://repo.radeon.com/rocm/rhel9/${ROCM_VERSION}/main\n\
enabled=1\n\
priority=50\n\
gpgcheck=1\n\
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key" > /etc/yum.repos.d/amdgpu.repo


RUN --mount=type=cache,target=/root/.cache/uv \
    export version="$(awk -F. '{print $1"."$2}' <<< $ROCM_VERSION)" && \
    uv pip install --pre \
        --index-url "https://download.pytorch.org/whl/rocm${version}" \
        torch==2.7.0+rocm${version}\
        torchvision==0.22.0+rocm${version} && \
    # Install libdrm-amdgpu to avoid errors when retrieving device information (amdgpu.ids: No such file or directory)
    microdnf install -y --nodocs libdrm-amdgpu && \
    microdnf clean all


ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/numpy.libs:$LD_LIBRARY_PATH"
ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/pillow.libs:$LD_LIBRARY_PATH"
ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/triton/backends/amd/lib:$LD_LIBRARY_PATH"
ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/torch/lib:$LD_LIBRARY_PATH"

RUN echo $LD_LIBRARY_PATH | tr : \\n >> /etc/ld.so.conf.d/torch-venv.conf && \
    ldconfig

FROM rocm_base as build_amdsmi

RUN microdnf -y install \
    amd-smi-lib && \
    microdnf clean all

WORKDIR /opt/rocm/share/amd_smi

RUN python setup.py bdist_wheel --dist-dir=/dist/

##################################################################################################

FROM rocm_base AS vllm-openai
ARG FLASH_ATTENTION_WHEEL_STRATEGY
ARG VLLM_WHEEL_STRATEGY

WORKDIR /workspace

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH=$VIRTUAL_ENV/bin:$PATH

# Required for triton
RUN microdnf install -y --setopt=install_weak_deps=0 --nodocs gcc rsync && \
    microdnf clean all

RUN --mount=type=bind,from=build_amdsmi,src=/dist,target=/install/amdsmi/ \
    --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,src=payload,target=/workspace/payload \
    ./payload/run.sh

ENV HF_HUB_OFFLINE=1 \
    HOME=/home/vllm \
    # Allow requested max length to exceed what is extracted from the
    # config.json
    # see: https://github.com/vllm-project/vllm/pull/7080
    VLLM_USAGE_SOURCE=production-docker-image \
    VLLM_WORKER_MULTIPROC_METHOD=spawn \
    VLLM_NO_USAGE_STATS=1 \
    # Silences the HF Tokenizers warning
    TOKENIZERS_PARALLELISM=false  \
    RAY_EXPERIMENTAL_NOSET_ROCR_VISIBLE_DEVICES=1 \
    VLLM_USE_TRITON_FLASH_ATTN=0 \
    VLLM_USE_V1=1 \
    HIP_FORCE_DEV_KERNARG=1 \
    OUTLINES_CACHE_DIR=/tmp/outlines \
    NUMBA_CACHE_DIR=/tmp/numba \
    TRITON_CACHE_DIR=/tmp/triton

# setup non-root user for OpenShift
RUN umask 002 && \
    useradd --uid 2000 --gid 0 vllm && \
    mkdir -p /licenses /home/vllm && \
    chmod g+rwx /home/vllm

COPY LICENSE /licenses/vllm.md
COPY examples/*.jinja /app/data/template/
RUN mkdir -p /opt/app-root && \
    ln -s /app/data/template /opt/app-root/template

USER 2000
WORKDIR /home/vllm

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]


FROM vllm-openai as vllm-grpc-adapter
ARG VLLM_TGIS_ADAPTER_VERSION

USER root

RUN --mount=type=bind,from=build_amdsmi,src=/dist,target=/install/amdsmi/ \
    --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,src=payload,target=/workspace/payload \
    cd /workspace && \
    env HOME=/root VLLM_TGIS_ADAPTER_VERSION=${VLLM_TGIS_ADAPTER_VERSION} \
        ./payload/run.sh

ENV GRPC_PORT=8033 \
    PORT=8000 \
    # As an optimization, vLLM disables logprobs when using spec decoding by
    # default, but this would be unexpected to users of a hosted model that
    # happens to have spec decoding
    # see: https://github.com/vllm-project/vllm/pull/6485
    DISABLE_LOGPROBS_DURING_SPEC_DECODING=false

USER 2000
ENTRYPOINT ["python3", "-m", "vllm_tgis_adapter", "--uvicorn-log-level=warning"]
